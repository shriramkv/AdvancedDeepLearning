{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Load a public, lightweight model (facebook/opt-1.3b)\n",
        "model_name = \"facebook/opt-1.3b\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\").eval()\n",
        "\n",
        "# Define input text\n",
        "input_text = \"The future of AI is\"\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "# Fix missing pad_token_id warning\n",
        "tokenizer.pad_token = tokenizer.eos_token  # Set padding token to EOS token\n",
        "\n",
        "# Move tensors to the appropriate device (GPU or CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "# Without KV Cache\n",
        "start_time = time.time()\n",
        "output_no_cache = model.generate(\n",
        "    inputs[\"input_ids\"],\n",
        "    attention_mask=inputs[\"attention_mask\"],\n",
        "    max_length=100,\n",
        "    use_cache=False,\n",
        "    do_sample=True,\n",
        "    top_k=50,\n",
        "    temperature=0.7,\n",
        "    repetition_penalty=1.2\n",
        ")\n",
        "time_without_cache = (time.time() - start_time) * 1000  # Convert to ms\n",
        "\n",
        "# With KV Cache\n",
        "start_time = time.time()\n",
        "output_with_cache = model.generate(\n",
        "    inputs[\"input_ids\"],\n",
        "    attention_mask=inputs[\"attention_mask\"],\n",
        "    max_length=100,\n",
        "    use_cache=True,\n",
        "    do_sample=True,\n",
        "    top_k=50,\n",
        "    temperature=0.7,\n",
        "    repetition_penalty=1.2\n",
        ")\n",
        "time_with_cache = (time.time() - start_time) * 1000  # Convert to ms\n",
        "\n",
        "# Print results\n",
        "print(f\"Time without KV Cache: {time_without_cache:.2f} ms\")\n",
        "print(f\"Time with KV Cache: {time_with_cache:.2f} ms\")\n",
        "print(f\"Speed Improvement: {time_without_cache / time_with_cache:.2f}x\\n\")\n",
        "\n",
        "# Print generated text\n",
        "print(\"\\nGenerated Text Without Cache:\", tokenizer.decode(output_no_cache[0], skip_special_tokens=True))\n",
        "print(\"Generated Text With Cache:\", tokenizer.decode(output_with_cache[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "id": "vG1Ll2LzDVww",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0aad325a-3131-490d-be97-7cd6f14f8c29"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time without KV Cache: 2036.86 ms\n",
            "Time with KV Cache: 1317.99 ms\n",
            "Speed Improvement: 1.55x\n",
            "\n",
            "\n",
            "Generated Text Without Cache: The future of AI is in the cloud\n",
            "\n",
            "I know I am going to get a lot of questions from people who are new or have not heard this before. But, if you haven’t seen it yet… The technology we use for machine learning and deep reinforcement learning will be around forever. It has been around since the 1960s and when computers were first invented they did many things that humans couldn't do at all (at least not really). In fact, computers could even play\n",
            "Generated Text With Cache: The future of AI is here, and it's terrifying\n",
            "In 2019, Google CEO Sundar Pichai said that his company would invest $15 billion in AI over the next two years. So far this year, Google has made moves to bolster its neural networks research team, adding more engineers than ever before, according to CNBC. And now, Microsoft has announced a new program called Project Baseline that aims to create a platform for developers with tools like machine learning frameworks and data pipelines across all\n"
          ]
        }
      ]
    }
  ]
}